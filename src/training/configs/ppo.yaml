algorithm: PPO
vision: true
total_timesteps: 50000
agent_kwargs:
  learning_rate: 0.0003
  n_steps: 512        # Reduced for faster updates
  batch_size: 64      # Smaller batch size for CPU
  n_epochs: 5         # Fewer epochs per update
  gamma: 0.99
  ent_coef: 0.01     # Entropy coefficient for exploration
  clip_range: 0.2    # PPO clip range
  policy_kwargs:
    net_arch:
      pi: [128, 64]   # Policy network architecture
      vf: [128, 64]   # Value function network architecture
    features_extractor_kwargs:
      features_dim: 512  # Dimens√£o final das features combinadas
    normalize_images: true  # Normaliza as imagens automaticamente